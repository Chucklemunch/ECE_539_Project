{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5db74075",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:41.656255200Z",
          "start_time": "2023-12-02T18:14:37.564197800Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5db74075",
        "outputId": "ccd9b840-c9da-4b54-a036-1b550518890f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.3-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.3\n"
          ]
        }
      ],
      "source": [
        "pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1504d98a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:41.699140600Z",
          "start_time": "2023-12-02T18:14:41.660242900Z"
        },
        "id": "1504d98a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "16e4b17d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:46.769339600Z",
          "start_time": "2023-12-02T18:14:41.674207500Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16e4b17d",
        "outputId": "93faa913-2cce-4dfa-aee8-166efe6ef8fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 891, 'name': 'CDC Diabetes Health Indicators', 'repository_url': 'https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators', 'data_url': 'https://archive.ics.uci.edu/static/public/891/data.csv', 'abstract': 'The Diabetes Health Indicators Dataset contains healthcare statistics and lifestyle survey information about people in general along with their diagnosis of diabetes. The 35 features consist of some demographics, lab test results, and answers to survey questions for each patient. The target variable for classification is whether a patient has diabetes, is pre-diabetic, or healthy. ', 'area': 'Health and Medicine', 'tasks': ['Classification'], 'characteristics': ['Tabular', 'Multivariate'], 'num_instances': 253680, 'num_features': 21, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Sex', 'Age', 'Education Level', 'Income'], 'target_col': ['Diabetes_binary'], 'index_col': ['ID'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2017, 'last_updated': 'Fri Nov 03 2023', 'dataset_doi': '10.24432/C53919', 'creators': [], 'intro_paper': {'title': 'Incidence of End-Stage Renal Disease Attributed to Diabetes Among Persons with Diagnosed Diabetes — United States and Puerto Rico, 2000–2014', 'authors': 'Nilka Rios Burrows, MPH; Israel Hora, PhD; Linda S. Geiss, MA; Edward W. Gregg, PhD; Ann Albright, PhD', 'published_in': 'Morbidity and Mortality Weekly Report', 'year': 2017, 'url': 'https://www.cdc.gov/mmwr/volumes/66/wr/mm6643a2.htm', 'doi': None}, 'additional_info': {'summary': 'Dataset link: https://www.cdc.gov/brfss/annual_data/annual_2014.html', 'purpose': 'To better understand the relationship between  lifestyle and diabetes in the US', 'funded_by': 'The CDC', 'instances_represent': 'Each row represents a person participating in this study.', 'recommended_data_splits': 'Cross validation or a fixed train-test split could be used.', 'sensitive_data': '- Gender\\n- Income\\n- Education level', 'preprocessing_description': 'Bucketing of age', 'variable_info': '- Diabetes diagnosis\\n- Demographics (race, sex)\\n- Personal information (income, educations)\\n- Health history (drinking, smoking, mental health, physical health)', 'citation': None}, 'external_url': 'https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset'}\n",
            "                    name     role     type      demographic  \\\n",
            "0                     ID       ID  Integer             None   \n",
            "1        Diabetes_binary   Target   Binary             None   \n",
            "2                 HighBP  Feature   Binary             None   \n",
            "3               HighChol  Feature   Binary             None   \n",
            "4              CholCheck  Feature   Binary             None   \n",
            "5                    BMI  Feature  Integer             None   \n",
            "6                 Smoker  Feature   Binary             None   \n",
            "7                 Stroke  Feature   Binary             None   \n",
            "8   HeartDiseaseorAttack  Feature   Binary             None   \n",
            "9           PhysActivity  Feature   Binary             None   \n",
            "10                Fruits  Feature   Binary             None   \n",
            "11               Veggies  Feature   Binary             None   \n",
            "12     HvyAlcoholConsump  Feature   Binary             None   \n",
            "13         AnyHealthcare  Feature   Binary             None   \n",
            "14           NoDocbcCost  Feature   Binary             None   \n",
            "15               GenHlth  Feature  Integer             None   \n",
            "16              MentHlth  Feature  Integer             None   \n",
            "17              PhysHlth  Feature  Integer             None   \n",
            "18              DiffWalk  Feature   Binary             None   \n",
            "19                   Sex  Feature   Binary              Sex   \n",
            "20                   Age  Feature  Integer              Age   \n",
            "21             Education  Feature  Integer  Education Level   \n",
            "22                Income  Feature  Integer           Income   \n",
            "\n",
            "                                          description units missing_values  \n",
            "0                                          Patient ID  None             no  \n",
            "1         0 = no diabetes 1 = prediabetes or diabetes  None             no  \n",
            "2                          0 = no high BP 1 = high BP  None             no  \n",
            "3        0 = no high cholesterol 1 = high cholesterol  None             no  \n",
            "4   0 = no cholesterol check in 5 years 1 = yes ch...  None             no  \n",
            "5                                     Body Mass Index  None             no  \n",
            "6   Have you smoked at least 100 cigarettes in you...  None             no  \n",
            "7        (Ever told) you had a stroke. 0 = no 1 = yes  None             no  \n",
            "8   coronary heart disease (CHD) or myocardial inf...  None             no  \n",
            "9   physical activity in past 30 days - not includ...  None             no  \n",
            "10  Consume Fruit 1 or more times per day 0 = no 1...  None             no  \n",
            "11  Consume Vegetables 1 or more times per day 0 =...  None             no  \n",
            "12  Heavy drinkers (adult men having more than 14 ...  None             no  \n",
            "13  Have any kind of health care coverage, includi...  None             no  \n",
            "14  Was there a time in the past 12 months when yo...  None             no  \n",
            "15  Would you say that in general your health is: ...  None             no  \n",
            "16  Now thinking about your mental health, which i...  None             no  \n",
            "17  Now thinking about your physical health, which...  None             no  \n",
            "18  Do you have serious difficulty walking or clim...  None             no  \n",
            "19                                0 = female 1 = male  None             no  \n",
            "20  13-level age category (_AGEG5YR see codebook) ...  None             no  \n",
            "21  Education level (EDUCA see codebook) scale 1-6...  None             no  \n",
            "22  Income scale (INCOME2 see codebook) scale 1-8 ...  None             no  \n"
          ]
        }
      ],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import ssl\n",
        "\n",
        "# Ignore ssl certificate verification\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# fetch dataset\n",
        "cdc_diabetes_health_indicators = fetch_ucirepo(id=891)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X_dataframe = cdc_diabetes_health_indicators.data.features\n",
        "y_dataframe = cdc_diabetes_health_indicators.data.targets\n",
        "\n",
        "# metadata\n",
        "print(cdc_diabetes_health_indicators.metadata)\n",
        "\n",
        "# variable information\n",
        "print(cdc_diabetes_health_indicators.variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f664478f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:46.787290300Z",
          "start_time": "2023-12-02T18:14:46.767344400Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f664478f",
        "outputId": "6780d340-1867-43c9-b031-4f038ce6fa50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "torch.Size([253680, 21]) torch.Size([253680, 1])\n"
          ]
        }
      ],
      "source": [
        "# Converting DataFrames to Pytorch tensors... because who wants to learn Pandas right now?\n",
        "X = torch.from_numpy(X_dataframe.to_numpy())\n",
        "y = torch.from_numpy(y_dataframe.to_numpy())\n",
        "\n",
        "print(type(X), type(y))\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e2cb5890",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:48.339141700Z",
          "start_time": "2023-12-02T18:14:46.785297100Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2cb5890",
        "outputId": "1d856018-96ab-41ff-8d41-cbc5e045a45e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "X_test: torch.Size([38052, 21]) X_val: torch.Size([38052, 21]) X_train: torch.Size([177576, 21])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Converting labels to 1-hot encoding\n",
        "def to1hot(labels):\n",
        "    return torch.eye(2)[labels]\n",
        "\n",
        "labels = to1hot(y[:,0])\n",
        "print(labels[:3])\n",
        "\n",
        "# Splitting data into train, validation, and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=.3, stratify=labels, shuffle=True, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=.5, stratify=y_test, shuffle=True, random_state=42)\n",
        "print(\"X_test:\", X_test.shape, \"X_val:\", X_val.shape, \"X_train:\", X_train.shape)\n",
        "\n",
        "# Standardizing features\n",
        "standardizer = StandardScaler()\n",
        "X_train = torch.from_numpy(standardizer.fit_transform(X_train))\n",
        "X_val = torch.from_numpy(standardizer.transform(X_val))\n",
        "X_test = torch.from_numpy(standardizer.transform(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e25d430d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:48.372053500Z",
          "start_time": "2023-12-02T18:14:48.343130200Z"
        },
        "id": "e25d430d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Making Datasets for training, validation, and testing\n",
        "class HealthData(Dataset):\n",
        "    def __init__(self, participants):\n",
        "        self.participants = participants\n",
        "\n",
        "    def __len__(self):\n",
        "        # The total number of particpants in the dataset\n",
        "        return len(self.participants)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Extract the particpant data\n",
        "        participant = self.participants[idx]\n",
        "        return participant\n",
        "\n",
        "\n",
        "# Each dataset is an R x 23 tensor, where the final 2 columns serve as a 1-hot encoding of the labels\n",
        "train_dataset = HealthData(torch.concat((X_train, y_train), dim=1).float())\n",
        "validation_dataset = HealthData(torch.concat((X_val, y_val), dim=1).float())\n",
        "test_dataset = HealthData(torch.concat((X_test, y_test), dim=1).float())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "de1f66c0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:48.419927700Z",
          "start_time": "2023-12-02T18:14:48.391004400Z"
        },
        "id": "de1f66c0"
      },
      "outputs": [],
      "source": [
        "# Define model with residual connections and implement batch normalization\n",
        "class BatchNormDNN(nn.Module):\n",
        "    def __init__(self, input_dim=21, hidden_dim=5, num_classes=2, num_layers=15, activation=nn.LeakyReLU):\n",
        "        super(BatchNormDNN, self).__init__()\n",
        "\n",
        "        # Define layers\n",
        "        layers = [nn.Linear(input_dim, hidden_dim)]\n",
        "        for i in range(1, num_layers-1):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dim, num_classes))\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.relu = activation()\n",
        "        self.batch_norm = torch.nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i == 0:\n",
        "                x = self.relu(self.batch_norm(layer(x)))\n",
        "            # Don't apply relu or batch_norm to last layer\n",
        "            elif i == len(self.layers) - 1:\n",
        "                x =  layer(x)\n",
        "            else:\n",
        "                # Residual connections\n",
        "                x = x + self.relu(self.batch_norm(layer(x)))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6b316672",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:20:12.039250300Z",
          "start_time": "2023-12-02T18:14:48.441866100Z"
        },
        "id": "6b316672"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(model, loss, dataloader, optimizer):\n",
        "    \"\"\"Helper function to train our model.\"\"\"\n",
        "    total_error = 0.\n",
        "    for it, batch in enumerate(dataloader):\n",
        "        inputs = batch[:, :-2]\n",
        "        labels = batch[:, -2:]\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Compute model predictions\n",
        "        pred = model(inputs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute the loss\n",
        "        l = loss(pred, labels)\n",
        "        total_error += l.item()\n",
        "\n",
        "        l.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_error / len(dataloader)\n",
        "\n",
        "\n",
        "def fit(model, loss, dataloader, epochs=41):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.03)\n",
        "    for ep in range(epochs):\n",
        "        err = train(model, loss, dataloader, optimizer)\n",
        "        if ep % 10 == 0:\n",
        "            print(f\"[Ep{ep}] Loss {err:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        inputs = batch[:, :-2]\n",
        "        labels = batch[:, -2:]\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Get model predictions\n",
        "        predictions = model(inputs)\n",
        "\n",
        "        # Collect predictions and labels\n",
        "        all_predictions.extend(torch.argmax(predictions, dim=1).tolist())\n",
        "        all_labels.extend(torch.argmax(labels, dim=1).tolist())\n",
        "\n",
        "    # Confusion matrix, Accuracy, Error\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    error = 1 - accuracy\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Error:\", error)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "ziUIDXDKDd3A"
      },
      "id": "ziUIDXDKDd3A",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Loss Function\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define data loader for easy batching\n",
        "dataloader = DataLoader(train_dataset, batch_size=10000, shuffle=True, drop_last=True)\n",
        "valloader = DataLoader(validation_dataset, batch_size=5000, shuffle=True, drop_last=True)\n",
        "\n",
        "# Model\n",
        "print(\"LeakyReLU activation\")\n",
        "leaky_relu_model = BatchNormDNN(num_layers=5, hidden_dim=30, activation=nn.LeakyReLU)\n",
        "leaky_relu_model = leaky_relu_model.to(device)\n",
        "fit(leaky_relu_model, loss, dataloader, epochs=50)\n",
        "print()\n",
        "evaluate_model(leaky_relu_model, valloader)\n",
        "print()\n",
        "\n",
        "print(\"Sigmoid activation\")\n",
        "sigmoid_model = BatchNormDNN(num_layers=5, hidden_dim=30, activation=nn.Sigmoid)\n",
        "sigmoid_model = sigmoid_model.to(device)\n",
        "fit(sigmoid_model, loss, dataloader, epochs=50)\n",
        "print()\n",
        "evaluate_model(sigmoid_model, valloader)\n",
        "print()\n",
        "\n",
        "print(\"TanH activation\")\n",
        "tanh_model = BatchNormDNN(num_layers=5, hidden_dim=30, activation=nn.Tanh)\n",
        "tanh_model = tanh_model.to(device)\n",
        "fit(tanh_model, loss, dataloader, epochs=50)\n",
        "print()\n",
        "evaluate_model(tanh_model, valloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ_muDg0Dkj6",
        "outputId": "87a3b77c-c60f-47b3-caa0-e645e984a70c"
      },
      "id": "PJ_muDg0Dkj6",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LeakyReLU activation\n",
            "[Ep0] Loss 0.374\n",
            "[Ep10] Loss 0.312\n",
            "[Ep20] Loss 0.310\n",
            "[Ep30] Loss 0.310\n",
            "[Ep40] Loss 0.309\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29658   469]\n",
            " [ 4116   757]]\n",
            "Accuracy: 0.869\n",
            "Error: 0.131\n",
            "\n",
            "Sigmoid activation\n",
            "[Ep0] Loss 0.446\n",
            "[Ep10] Loss 0.314\n",
            "[Ep20] Loss 0.311\n",
            "[Ep30] Loss 0.311\n",
            "[Ep40] Loss 0.311\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29564   558]\n",
            " [ 4068   810]]\n",
            "Accuracy: 0.8678285714285714\n",
            "Error: 0.1321714285714286\n",
            "\n",
            "TanH activation\n",
            "[Ep0] Loss 0.493\n",
            "[Ep10] Loss 0.314\n",
            "[Ep20] Loss 0.312\n",
            "[Ep30] Loss 0.312\n",
            "[Ep40] Loss 0.311\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29688   407]\n",
            " [ 4233   672]]\n",
            "Accuracy: 0.8674285714285714\n",
            "Error: 0.13257142857142856\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8674285714285714"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We can clearly see from the validation set that even though negligible, **LeakyReLU** has the best accuracy among all the different activation functions used!"
      ],
      "metadata": {
        "id": "oh4xYMxiGW9I"
      },
      "id": "oh4xYMxiGW9I"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz5beQ-C6kfn",
        "outputId": "dace259f-8b3c-42d8-8dfe-4f9ee4dae46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HIDDEN LAYERS = 10\n",
            "DIMENSIONS = 5\n",
            "[Ep0] Loss 0.382\n",
            "[Ep10] Loss 0.315\n",
            "[Ep20] Loss 0.313\n",
            "[Ep30] Loss 0.313\n",
            "[Ep40] Loss 0.313\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29678   468]\n",
            " [ 4121   733]]\n",
            "Accuracy: 0.8688857142857143\n",
            "Error: 0.13111428571428574\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 10\n",
            "DIMENSIONS = 10\n",
            "[Ep0] Loss 0.388\n",
            "[Ep10] Loss 0.313\n",
            "[Ep20] Loss 0.312\n",
            "[Ep30] Loss 0.312\n",
            "[Ep40] Loss 0.312\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29835   304]\n",
            " [ 4333   528]]\n",
            "Accuracy: 0.8675142857142857\n",
            "Error: 0.13248571428571432\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 10\n",
            "DIMENSIONS = 21\n",
            "[Ep0] Loss 0.427\n",
            "[Ep10] Loss 0.312\n",
            "[Ep20] Loss 0.310\n",
            "[Ep30] Loss 0.309\n",
            "[Ep40] Loss 0.310\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29548   555]\n",
            " [ 4066   831]]\n",
            "Accuracy: 0.8679714285714286\n",
            "Error: 0.13202857142857138\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 10\n",
            "DIMENSIONS = 50\n",
            "[Ep0] Loss 0.622\n",
            "[Ep10] Loss 0.311\n",
            "[Ep20] Loss 0.310\n",
            "[Ep30] Loss 0.308\n",
            "[Ep40] Loss 0.307\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29599   515]\n",
            " [ 4078   808]]\n",
            "Accuracy: 0.8687714285714285\n",
            "Error: 0.13122857142857147\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 10\n",
            "DIMENSIONS = 100\n",
            "[Ep0] Loss 0.962\n",
            "[Ep10] Loss 0.312\n",
            "[Ep20] Loss 0.310\n",
            "[Ep30] Loss 0.308\n",
            "[Ep40] Loss 0.306\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29644   428]\n",
            " [ 4246   682]]\n",
            "Accuracy: 0.8664571428571428\n",
            "Error: 0.13354285714285719\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 30\n",
            "DIMENSIONS = 5\n",
            "[Ep0] Loss 0.408\n",
            "[Ep10] Loss 0.315\n",
            "[Ep20] Loss 0.313\n",
            "[Ep30] Loss 0.313\n",
            "[Ep40] Loss 0.314\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29611   515]\n",
            " [ 4096   778]]\n",
            "Accuracy: 0.8682571428571428\n",
            "Error: 0.13174285714285716\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 30\n",
            "DIMENSIONS = 10\n",
            "[Ep0] Loss 0.448\n",
            "[Ep10] Loss 0.313\n",
            "[Ep20] Loss 0.314\n",
            "[Ep30] Loss 0.313\n",
            "[Ep40] Loss 0.313\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29620   525]\n",
            " [ 4106   749]]\n",
            "Accuracy: 0.8676857142857143\n",
            "Error: 0.13231428571428572\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 30\n",
            "DIMENSIONS = 21\n",
            "[Ep0] Loss 0.526\n",
            "[Ep10] Loss 0.314\n",
            "[Ep20] Loss 0.312\n",
            "[Ep30] Loss 0.311\n",
            "[Ep40] Loss 0.310\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29379   737]\n",
            " [ 3879  1005]]\n",
            "Accuracy: 0.8681142857142857\n",
            "Error: 0.13188571428571427\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 30\n",
            "DIMENSIONS = 50\n",
            "[Ep0] Loss 0.842\n",
            "[Ep10] Loss 0.313\n",
            "[Ep20] Loss 0.311\n",
            "[Ep30] Loss 0.308\n",
            "[Ep40] Loss 0.306\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29492   623]\n",
            " [ 4006   879]]\n",
            "Accuracy: 0.8677428571428571\n",
            "Error: 0.13225714285714285\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 30\n",
            "DIMENSIONS = 100\n",
            "[Ep0] Loss 6.024\n",
            "[Ep10] Loss 0.316\n",
            "[Ep20] Loss 0.312\n",
            "[Ep30] Loss 0.308\n",
            "[Ep40] Loss 0.306\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29579   520]\n",
            " [ 4159   742]]\n",
            "Accuracy: 0.8663142857142857\n",
            "Error: 0.1336857142857143\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 50\n",
            "DIMENSIONS = 5\n",
            "[Ep0] Loss 0.456\n",
            "[Ep10] Loss 0.317\n",
            "[Ep20] Loss 0.315\n",
            "[Ep30] Loss 0.314\n",
            "[Ep40] Loss 0.314\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29594   534]\n",
            " [ 4114   758]]\n",
            "Accuracy: 0.8672\n",
            "Error: 0.13280000000000003\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 50\n",
            "DIMENSIONS = 10\n",
            "[Ep0] Loss 0.568\n",
            "[Ep10] Loss 0.314\n",
            "[Ep20] Loss 0.313\n",
            "[Ep30] Loss 0.313\n",
            "[Ep40] Loss 0.312\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29649   486]\n",
            " [ 4135   730]]\n",
            "Accuracy: 0.8679714285714286\n",
            "Error: 0.13202857142857138\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 50\n",
            "DIMENSIONS = 21\n",
            "[Ep0] Loss 1.183\n",
            "[Ep10] Loss 0.315\n",
            "[Ep20] Loss 0.312\n",
            "[Ep30] Loss 0.311\n",
            "[Ep40] Loss 0.311\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29686   412]\n",
            " [ 4276   626]]\n",
            "Accuracy: 0.8660571428571429\n",
            "Error: 0.13394285714285714\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 50\n",
            "DIMENSIONS = 50\n",
            "[Ep0] Loss 2.195\n",
            "[Ep10] Loss 0.315\n",
            "[Ep20] Loss 0.311\n",
            "[Ep30] Loss 0.309\n",
            "[Ep40] Loss 0.308\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29632   472]\n",
            " [ 4165   731]]\n",
            "Accuracy: 0.8675142857142857\n",
            "Error: 0.13248571428571432\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 50\n",
            "DIMENSIONS = 100\n",
            "[Ep0] Loss 6.735\n",
            "[Ep10] Loss 0.319\n",
            "[Ep20] Loss 0.312\n",
            "[Ep30] Loss 0.309\n",
            "[Ep40] Loss 0.306\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29373   711]\n",
            " [ 3953   963]]\n",
            "Accuracy: 0.8667428571428571\n",
            "Error: 0.13325714285714285\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 100\n",
            "DIMENSIONS = 5\n",
            "[Ep0] Loss 0.531\n",
            "[Ep10] Loss 0.320\n",
            "[Ep20] Loss 0.315\n",
            "[Ep30] Loss 0.315\n",
            "[Ep40] Loss 0.313\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29559   582]\n",
            " [ 4041   818]]\n",
            "Accuracy: 0.8679142857142857\n",
            "Error: 0.13208571428571425\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 100\n",
            "DIMENSIONS = 10\n",
            "[Ep0] Loss 0.860\n",
            "[Ep10] Loss 0.334\n",
            "[Ep20] Loss 0.316\n",
            "[Ep30] Loss 0.314\n",
            "[Ep40] Loss 0.314\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29601   498]\n",
            " [ 4186   715]]\n",
            "Accuracy: 0.8661714285714286\n",
            "Error: 0.1338285714285714\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 100\n",
            "DIMENSIONS = 21\n",
            "[Ep0] Loss 2.854\n",
            "[Ep10] Loss 0.320\n",
            "[Ep20] Loss 0.315\n",
            "[Ep30] Loss 0.313\n",
            "[Ep40] Loss 0.311\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29496   606]\n",
            " [ 4058   840]]\n",
            "Accuracy: 0.8667428571428571\n",
            "Error: 0.13325714285714285\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 100\n",
            "DIMENSIONS = 50\n",
            "[Ep0] Loss 3.369\n",
            "[Ep10] Loss 0.316\n",
            "[Ep20] Loss 0.313\n",
            "[Ep30] Loss 0.311\n",
            "[Ep40] Loss 0.309\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29610   516]\n",
            " [ 4145   729]]\n",
            "Accuracy: 0.8668285714285714\n",
            "Error: 0.1331714285714286\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 100\n",
            "DIMENSIONS = 100\n",
            "[Ep0] Loss 7.425\n",
            "[Ep10] Loss 0.316\n",
            "[Ep20] Loss 0.311\n",
            "[Ep30] Loss 0.310\n",
            "[Ep40] Loss 0.308\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29618   515]\n",
            " [ 4121   746]]\n",
            "Accuracy: 0.8675428571428572\n",
            "Error: 0.13245714285714283\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "dataloader = DataLoader(train_dataset, batch_size=10000, shuffle=True, drop_last=True)\n",
        "accuracies = {}\n",
        "for i in [10, 30, 50, 100]:\n",
        "    for j in [5, 10, 21, 50, 100]:\n",
        "        print(f\"HIDDEN LAYERS = {i}\")\n",
        "        print(f\"DIMENSIONS = {j}\")\n",
        "        experiment_model = BatchNormDNN(num_layers=i, hidden_dim=j)\n",
        "        experiment_model = experiment_model.to(device)\n",
        "        fit(experiment_model, loss, dataloader, epochs=50)\n",
        "        print()\n",
        "        accuracy = evaluate_model(experiment_model, valloader)\n",
        "        print()\n",
        "        print()\n",
        "        accuracies[f\"{i},{j}\"] = accuracy"
      ],
      "id": "Nz5beQ-C6kfn"
    },
    {
      "cell_type": "code",
      "source": [
        "best_key = max(accuracies, key=accuracies.get)\n",
        "best_i, best_j = map(int, best_key.split(','))\n",
        "\n",
        "print(f\"Best hidden layers: {best_i}\")\n",
        "print(f\"Best dimensions: {best_j}\")\n",
        "\n",
        "# Fit the final model\n",
        "test_model_bn = BatchNormDNN(num_layers=best_i, hidden_dim=best_j).to(device)\n",
        "dataloader = DataLoader(train_dataset, batch_size=10000, shuffle=True, drop_last=True)\n",
        "fit(test_model_bn, loss, dataloader, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fzAjs5eIHcb",
        "outputId": "f5f68e56-b3c2-4642-c468-930cbd4bb10b"
      },
      "id": "7fzAjs5eIHcb",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hidden layers: 10\n",
            "Best dimensions: 5\n",
            "[Ep0] Loss 0.365\n",
            "[Ep10] Loss 0.314\n",
            "[Ep20] Loss 0.313\n",
            "[Ep30] Loss 0.313\n",
            "[Ep40] Loss 0.313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "dd301c39",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:20:13.393628700Z",
          "start_time": "2023-12-02T18:20:12.046231400Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd301c39",
        "outputId": "939d6d3c-35ae-4b93-a54a-34c72b23456e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BatchNormDNN(\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=21, out_features=5, bias=True)\n",
            "    (1-8): 8 x Linear(in_features=5, out_features=5, bias=True)\n",
            "    (9): Linear(in_features=5, out_features=2, bias=True)\n",
            "  )\n",
            "  (relu): LeakyReLU(negative_slope=0.01)\n",
            "  (batch_norm): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(test_model_bn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c2a9ac895d68833a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:20:13.716764Z",
          "start_time": "2023-12-02T18:20:13.416569Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2a9ac895d68833a",
        "outputId": "44e7102d-027a-42dc-a433-82e7bd8dec0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[29663   453]\n",
            " [ 4208   676]]\n",
            "Accuracy: 0.8668285714285714\n",
            "Error: 0.1331714285714286\n"
          ]
        }
      ],
      "source": [
        "test_acc = evaluate_model(test_model_bn, test_dataloader)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}