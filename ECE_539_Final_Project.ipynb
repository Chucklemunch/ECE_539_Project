{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5db74075",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:41.656255200Z",
          "start_time": "2023-12-02T18:14:37.564197800Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5db74075",
        "outputId": "5d31297f-b5f6-4097-e14f-7d3d4f3b5ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.10/dist-packages (0.0.3)\n"
          ]
        }
      ],
      "source": [
        "pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1504d98a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:41.699140600Z",
          "start_time": "2023-12-02T18:14:41.660242900Z"
        },
        "id": "1504d98a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "16e4b17d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:46.769339600Z",
          "start_time": "2023-12-02T18:14:41.674207500Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16e4b17d",
        "outputId": "b216830d-4d07-45b4-ab92-d09b393112aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 891, 'name': 'CDC Diabetes Health Indicators', 'repository_url': 'https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators', 'data_url': 'https://archive.ics.uci.edu/static/public/891/data.csv', 'abstract': 'The Diabetes Health Indicators Dataset contains healthcare statistics and lifestyle survey information about people in general along with their diagnosis of diabetes. The 35 features consist of some demographics, lab test results, and answers to survey questions for each patient. The target variable for classification is whether a patient has diabetes, is pre-diabetic, or healthy. ', 'area': 'Health and Medicine', 'tasks': ['Classification'], 'characteristics': ['Tabular', 'Multivariate'], 'num_instances': 253680, 'num_features': 21, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Sex', 'Age', 'Education Level', 'Income'], 'target_col': ['Diabetes_binary'], 'index_col': ['ID'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2017, 'last_updated': 'Fri Nov 03 2023', 'dataset_doi': '10.24432/C53919', 'creators': [], 'intro_paper': {'title': 'Incidence of End-Stage Renal Disease Attributed to Diabetes Among Persons with Diagnosed Diabetes — United States and Puerto Rico, 2000–2014', 'authors': 'Nilka Rios Burrows, MPH; Israel Hora, PhD; Linda S. Geiss, MA; Edward W. Gregg, PhD; Ann Albright, PhD', 'published_in': 'Morbidity and Mortality Weekly Report', 'year': 2017, 'url': 'https://www.cdc.gov/mmwr/volumes/66/wr/mm6643a2.htm', 'doi': None}, 'additional_info': {'summary': 'Dataset link: https://www.cdc.gov/brfss/annual_data/annual_2014.html', 'purpose': 'To better understand the relationship between  lifestyle and diabetes in the US', 'funded_by': 'The CDC', 'instances_represent': 'Each row represents a person participating in this study.', 'recommended_data_splits': 'Cross validation or a fixed train-test split could be used.', 'sensitive_data': '- Gender\\n- Income\\n- Education level', 'preprocessing_description': 'Bucketing of age', 'variable_info': '- Diabetes diagnosis\\n- Demographics (race, sex)\\n- Personal information (income, educations)\\n- Health history (drinking, smoking, mental health, physical health)', 'citation': None}, 'external_url': 'https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset'}\n",
            "                    name     role     type      demographic  \\\n",
            "0                     ID       ID  Integer             None   \n",
            "1        Diabetes_binary   Target   Binary             None   \n",
            "2                 HighBP  Feature   Binary             None   \n",
            "3               HighChol  Feature   Binary             None   \n",
            "4              CholCheck  Feature   Binary             None   \n",
            "5                    BMI  Feature  Integer             None   \n",
            "6                 Smoker  Feature   Binary             None   \n",
            "7                 Stroke  Feature   Binary             None   \n",
            "8   HeartDiseaseorAttack  Feature   Binary             None   \n",
            "9           PhysActivity  Feature   Binary             None   \n",
            "10                Fruits  Feature   Binary             None   \n",
            "11               Veggies  Feature   Binary             None   \n",
            "12     HvyAlcoholConsump  Feature   Binary             None   \n",
            "13         AnyHealthcare  Feature   Binary             None   \n",
            "14           NoDocbcCost  Feature   Binary             None   \n",
            "15               GenHlth  Feature  Integer             None   \n",
            "16              MentHlth  Feature  Integer             None   \n",
            "17              PhysHlth  Feature  Integer             None   \n",
            "18              DiffWalk  Feature   Binary             None   \n",
            "19                   Sex  Feature   Binary              Sex   \n",
            "20                   Age  Feature  Integer              Age   \n",
            "21             Education  Feature  Integer  Education Level   \n",
            "22                Income  Feature  Integer           Income   \n",
            "\n",
            "                                          description units missing_values  \n",
            "0                                          Patient ID  None             no  \n",
            "1         0 = no diabetes 1 = prediabetes or diabetes  None             no  \n",
            "2                          0 = no high BP 1 = high BP  None             no  \n",
            "3        0 = no high cholesterol 1 = high cholesterol  None             no  \n",
            "4   0 = no cholesterol check in 5 years 1 = yes ch...  None             no  \n",
            "5                                     Body Mass Index  None             no  \n",
            "6   Have you smoked at least 100 cigarettes in you...  None             no  \n",
            "7        (Ever told) you had a stroke. 0 = no 1 = yes  None             no  \n",
            "8   coronary heart disease (CHD) or myocardial inf...  None             no  \n",
            "9   physical activity in past 30 days - not includ...  None             no  \n",
            "10  Consume Fruit 1 or more times per day 0 = no 1...  None             no  \n",
            "11  Consume Vegetables 1 or more times per day 0 =...  None             no  \n",
            "12  Heavy drinkers (adult men having more than 14 ...  None             no  \n",
            "13  Have any kind of health care coverage, includi...  None             no  \n",
            "14  Was there a time in the past 12 months when yo...  None             no  \n",
            "15  Would you say that in general your health is: ...  None             no  \n",
            "16  Now thinking about your mental health, which i...  None             no  \n",
            "17  Now thinking about your physical health, which...  None             no  \n",
            "18  Do you have serious difficulty walking or clim...  None             no  \n",
            "19                                0 = female 1 = male  None             no  \n",
            "20  13-level age category (_AGEG5YR see codebook) ...  None             no  \n",
            "21  Education level (EDUCA see codebook) scale 1-6...  None             no  \n",
            "22  Income scale (INCOME2 see codebook) scale 1-8 ...  None             no  \n"
          ]
        }
      ],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import ssl\n",
        "\n",
        "# Ignore ssl certificate verification\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# fetch dataset\n",
        "cdc_diabetes_health_indicators = fetch_ucirepo(id=891)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X_dataframe = cdc_diabetes_health_indicators.data.features\n",
        "y_dataframe = cdc_diabetes_health_indicators.data.targets\n",
        "\n",
        "# metadata\n",
        "print(cdc_diabetes_health_indicators.metadata)\n",
        "\n",
        "# variable information\n",
        "print(cdc_diabetes_health_indicators.variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f664478f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:46.787290300Z",
          "start_time": "2023-12-02T18:14:46.767344400Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f664478f",
        "outputId": "362e1278-7fd8-412d-8db0-6c5e924b4e85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "torch.Size([253680, 21]) torch.Size([253680, 1])\n"
          ]
        }
      ],
      "source": [
        "# Converting DataFrames to Pytorch tensors... because who wants to learn Pandas right now?\n",
        "X = torch.from_numpy(X_dataframe.to_numpy())\n",
        "y = torch.from_numpy(y_dataframe.to_numpy())\n",
        "\n",
        "print(type(X), type(y))\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e2cb5890",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:48.339141700Z",
          "start_time": "2023-12-02T18:14:46.785297100Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2cb5890",
        "outputId": "e52f4a32-5620-4419-c890-993525ffe0e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "X_test: torch.Size([38052, 21]) X_val: torch.Size([38052, 21]) X_train: torch.Size([177576, 21])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Converting labels to 1-hot encoding\n",
        "def to1hot(labels):\n",
        "    return torch.eye(2)[labels]\n",
        "\n",
        "labels = to1hot(y[:,0])\n",
        "print(labels[:3])\n",
        "\n",
        "# Splitting data into train, validation, and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=.3, stratify=labels, shuffle=True, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=.5, stratify=y_test, shuffle=True, random_state=42)\n",
        "print(\"X_test:\", X_test.shape, \"X_val:\", X_val.shape, \"X_train:\", X_train.shape)\n",
        "\n",
        "# Standardizing features\n",
        "standardizer = StandardScaler()\n",
        "X_train = torch.from_numpy(standardizer.fit_transform(X_train))\n",
        "X_val = torch.from_numpy(standardizer.transform(X_val))\n",
        "X_test = torch.from_numpy(standardizer.transform(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e25d430d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:48.372053500Z",
          "start_time": "2023-12-02T18:14:48.343130200Z"
        },
        "id": "e25d430d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Making Datasets for training, validation, and testing\n",
        "class HealthData(Dataset):\n",
        "    def __init__(self, participants):\n",
        "        self.participants = participants\n",
        "\n",
        "    def __len__(self):\n",
        "        # The total number of particpants in the dataset\n",
        "        return len(self.participants)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Extract the particpant data\n",
        "        participant = self.participants[idx]\n",
        "        return participant\n",
        "\n",
        "\n",
        "# Each dataset is an R x 23 tensor, where the final 2 columns serve as a 1-hot encoding of the labels\n",
        "train_dataset = HealthData(torch.concat((X_train, y_train), dim=1).float())\n",
        "validation_dataset = HealthData(torch.concat((X_val, y_val), dim=1).float())\n",
        "test_dataset = HealthData(torch.concat((X_test, y_test), dim=1).float())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "de1f66c0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:14:48.419927700Z",
          "start_time": "2023-12-02T18:14:48.391004400Z"
        },
        "id": "de1f66c0"
      },
      "outputs": [],
      "source": [
        "# Define model with residual connections and implement batch normalization\n",
        "class BatchNormDNN(nn.Module):\n",
        "    def __init__(self, input_dim=21, hidden_dim=5, num_classes=2, num_layers=15, activation=nn.LeakyReLU):\n",
        "        super(BatchNormDNN, self).__init__()\n",
        "\n",
        "        # Define layers\n",
        "        layers = [nn.Linear(input_dim, hidden_dim)]\n",
        "        for i in range(1, num_layers-1):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dim, num_classes))\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.relu = activation()\n",
        "        self.batch_norm = torch.nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i == 0:\n",
        "                x = self.relu(self.batch_norm(layer(x)))\n",
        "            # Don't apply relu or batch_norm to last layer\n",
        "            elif i == len(self.layers) - 1:\n",
        "                x =  layer(x)\n",
        "            else:\n",
        "                # Residual connections\n",
        "                x = x + self.relu(self.batch_norm(layer(x)))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6b316672",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:20:12.039250300Z",
          "start_time": "2023-12-02T18:14:48.441866100Z"
        },
        "id": "6b316672"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(model, loss, dataloader, optimizer):\n",
        "    \"\"\"Helper function to train our model.\"\"\"\n",
        "    total_error = 0.\n",
        "    for it, batch in enumerate(dataloader):\n",
        "        inputs = batch[:, :-2]\n",
        "        labels = batch[:, -2:]\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Compute model predictions\n",
        "        pred = model(inputs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute the loss\n",
        "        l = loss(pred, labels)\n",
        "        total_error += l.item()\n",
        "\n",
        "        l.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_error / len(dataloader)\n",
        "\n",
        "\n",
        "def fit(model, loss, dataloader, epochs=41):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.03)\n",
        "    for ep in range(epochs):\n",
        "        err = train(model, loss, dataloader, optimizer)\n",
        "        if ep % 10 == 0:\n",
        "            print(f\"[Ep{ep}] Loss {err:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        inputs = batch[:, :-2]\n",
        "        labels = batch[:, -2:]\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Get model predictions\n",
        "        predictions = model(inputs)\n",
        "\n",
        "        # Collect predictions and labels\n",
        "        all_predictions.extend(torch.argmax(predictions, dim=1).tolist())\n",
        "        all_labels.extend(torch.argmax(labels, dim=1).tolist())\n",
        "\n",
        "    # Confusion matrix, Accuracy, Error\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    error = 1 - accuracy\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Error:\", error)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "ziUIDXDKDd3A"
      },
      "id": "ziUIDXDKDd3A",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Loss Function\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define data loader for easy batching\n",
        "dataloader = DataLoader(train_dataset, batch_size=10000, shuffle=True, drop_last=True)\n",
        "valloader = DataLoader(validation_dataset, batch_size=5000, shuffle=True, drop_last=True)\n",
        "\n",
        "# Model\n",
        "print(\"LeakyReLU activation\")\n",
        "leaky_relu_model = BatchNormDNN(num_layers=5, hidden_dim=30, activation=nn.LeakyReLU)\n",
        "leaky_relu_model = leaky_relu_model.to(device)\n",
        "fit(leaky_relu_model, loss, dataloader, epochs=50)\n",
        "print()\n",
        "evaluate_model(leaky_relu_model, valloader)\n",
        "\n",
        "print(\"Sigmoid activation\")\n",
        "sigmoid_model = BatchNormDNN(num_layers=5, hidden_dim=30, activation=nn.Sigmoid)\n",
        "sigmoid_model = sigmoid_model.to(device)\n",
        "fit(sigmoid_model, loss, dataloader, epochs=50)\n",
        "print()\n",
        "evaluate_model(sigmoid_model, valloader)\n",
        "\n",
        "print(\"TanH activation\")\n",
        "tanh_model = BatchNormDNN(num_layers=5, hidden_dim=30, activation=nn.Tanh)\n",
        "tanh_model = tanh_model.to(device)\n",
        "fit(tanh_model, loss, dataloader, epochs=50)\n",
        "print()\n",
        "evaluate_model(tanh_model, valloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ_muDg0Dkj6",
        "outputId": "93618678-68d3-430d-e478-9ed181e84981"
      },
      "id": "PJ_muDg0Dkj6",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LeakyReLU activation\n",
            "[Ep0] Loss 0.381\n",
            "[Ep10] Loss 0.312\n",
            "[Ep20] Loss 0.310\n",
            "[Ep30] Loss 0.310\n",
            "[Ep40] Loss 0.309\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29566   571]\n",
            " [ 4034   829]]\n",
            "Accuracy: 0.8684285714285714\n",
            "Error: 0.13157142857142856\n",
            "Sigmoid activation\n",
            "[Ep0] Loss 0.436\n",
            "[Ep10] Loss 0.313\n",
            "[Ep20] Loss 0.311\n",
            "[Ep30] Loss 0.311\n",
            "[Ep40] Loss 0.311\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29753   406]\n",
            " [ 4189   652]]\n",
            "Accuracy: 0.8687142857142857\n",
            "Error: 0.13128571428571434\n",
            "TanH activation\n",
            "[Ep0] Loss 0.465\n",
            "[Ep10] Loss 0.313\n",
            "[Ep20] Loss 0.312\n",
            "[Ep30] Loss 0.311\n",
            "[Ep40] Loss 0.312\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29605   564]\n",
            " [ 4016   815]]\n",
            "Accuracy: 0.8691428571428571\n",
            "Error: 0.1308571428571429\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8691428571428571"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We can clearly see from the validation set that even though negligible, **LeakyReLU** has the best accuracy among all the different activation functions used!"
      ],
      "metadata": {
        "id": "oh4xYMxiGW9I"
      },
      "id": "oh4xYMxiGW9I"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz5beQ-C6kfn",
        "outputId": "9966c64e-66c7-4b69-89db-436ab7e43a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HIDDEN LAYERS = 10\n",
            "DIMENSIONS = 5\n",
            "[Ep0] Loss 0.374\n",
            "[Ep10] Loss 0.314\n",
            "[Ep20] Loss 0.314\n",
            "[Ep30] Loss 0.313\n",
            "[Ep40] Loss 0.313\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29446   667]\n",
            " [ 4004   883]]\n",
            "Accuracy: 0.8665428571428572\n",
            "Error: 0.13345714285714283\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 10\n",
            "DIMENSIONS = 10\n",
            "[Ep0] Loss 0.394\n",
            "[Ep10] Loss 0.313\n",
            "[Ep20] Loss 0.312\n",
            "[Ep30] Loss 0.312\n",
            "[Ep40] Loss 0.311\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29691   454]\n",
            " [ 4108   747]]\n",
            "Accuracy: 0.8696571428571429\n",
            "Error: 0.1303428571428571\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 10\n",
            "DIMENSIONS = 21\n",
            "[Ep0] Loss 0.391\n",
            "[Ep10] Loss 0.312\n",
            "[Ep20] Loss 0.312\n",
            "[Ep30] Loss 0.311\n",
            "[Ep40] Loss 0.309\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29683   452]\n",
            " [ 4205   660]]\n",
            "Accuracy: 0.8669428571428571\n",
            "Error: 0.13305714285714287\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 10\n",
            "DIMENSIONS = 50\n",
            "[Ep0] Loss 0.559\n",
            "[Ep10] Loss 0.311\n",
            "[Ep20] Loss 0.309\n",
            "[Ep30] Loss 0.307\n",
            "[Ep40] Loss 0.305\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29504   638]\n",
            " [ 3979   879]]\n",
            "Accuracy: 0.8680857142857142\n",
            "Error: 0.13191428571428576\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 10\n",
            "DIMENSIONS = 100\n",
            "[Ep0] Loss 0.851\n",
            "[Ep10] Loss 0.312\n",
            "[Ep20] Loss 0.309\n",
            "[Ep30] Loss 0.308\n",
            "[Ep40] Loss 0.304\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29473   649]\n",
            " [ 4053   825]]\n",
            "Accuracy: 0.8656571428571429\n",
            "Error: 0.1343428571428571\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 30\n",
            "DIMENSIONS = 5\n",
            "[Ep0] Loss 0.435\n",
            "[Ep10] Loss 0.317\n",
            "[Ep20] Loss 0.314\n",
            "[Ep30] Loss 0.314\n",
            "[Ep40] Loss 0.313\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29904   216]\n",
            " [ 4490   390]]\n",
            "Accuracy: 0.8655428571428572\n",
            "Error: 0.13445714285714283\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 30\n",
            "DIMENSIONS = 10\n",
            "[Ep0] Loss 0.541\n",
            "[Ep10] Loss 0.315\n",
            "[Ep20] Loss 0.313\n",
            "[Ep30] Loss 0.312\n",
            "[Ep40] Loss 0.311\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29679   444]\n",
            " [ 4183   694]]\n",
            "Accuracy: 0.8678\n",
            "Error: 0.13219999999999998\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 30\n",
            "DIMENSIONS = 21\n",
            "[Ep0] Loss 0.532\n",
            "[Ep10] Loss 0.314\n",
            "[Ep20] Loss 0.312\n",
            "[Ep30] Loss 0.311\n",
            "[Ep40] Loss 0.311\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29532   612]\n",
            " [ 4015   841]]\n",
            "Accuracy: 0.8678\n",
            "Error: 0.13219999999999998\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 30\n",
            "DIMENSIONS = 50\n",
            "[Ep0] Loss 1.517\n",
            "[Ep10] Loss 0.314\n",
            "[Ep20] Loss 0.311\n",
            "[Ep30] Loss 0.309\n",
            "[Ep40] Loss 0.308\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29587   539]\n",
            " [ 4080   794]]\n",
            "Accuracy: 0.8680285714285715\n",
            "Error: 0.13197142857142852\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 30\n",
            "DIMENSIONS = 100\n",
            "[Ep0] Loss 3.531\n",
            "[Ep10] Loss 0.315\n",
            "[Ep20] Loss 0.310\n",
            "[Ep30] Loss 0.308\n",
            "[Ep40] Loss 0.305\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29692   426]\n",
            " [ 4201   681]]\n",
            "Accuracy: 0.8678\n",
            "Error: 0.13219999999999998\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 50\n",
            "DIMENSIONS = 5\n",
            "[Ep0] Loss 0.410\n",
            "[Ep10] Loss 0.319\n",
            "[Ep20] Loss 0.316\n",
            "[Ep30] Loss 0.314\n",
            "[Ep40] Loss 0.313\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29752   378]\n",
            " [ 4289   581]]\n",
            "Accuracy: 0.8666571428571429\n",
            "Error: 0.1333428571428571\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 50\n",
            "DIMENSIONS = 10\n",
            "[Ep0] Loss 0.586\n",
            "[Ep10] Loss 0.319\n",
            "[Ep20] Loss 0.314\n",
            "[Ep30] Loss 0.312\n",
            "[Ep40] Loss 0.313\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29462   642]\n",
            " [ 4015   881]]\n",
            "Accuracy: 0.8669428571428571\n",
            "Error: 0.13305714285714287\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 50\n",
            "DIMENSIONS = 21\n",
            "[Ep0] Loss 0.891\n",
            "[Ep10] Loss 0.315\n",
            "[Ep20] Loss 0.313\n",
            "[Ep30] Loss 0.312\n",
            "[Ep40] Loss 0.310\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29534   630]\n",
            " [ 3988   848]]\n",
            "Accuracy: 0.8680571428571429\n",
            "Error: 0.13194285714285714\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 50\n",
            "DIMENSIONS = 50\n",
            "[Ep0] Loss 2.125\n",
            "[Ep10] Loss 0.314\n",
            "[Ep20] Loss 0.310\n",
            "[Ep30] Loss 0.309\n",
            "[Ep40] Loss 0.308\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29463   680]\n",
            " [ 3938   919]]\n",
            "Accuracy: 0.8680571428571429\n",
            "Error: 0.13194285714285714\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 50\n",
            "DIMENSIONS = 100\n",
            "[Ep0] Loss 3.394\n",
            "[Ep10] Loss 0.315\n",
            "[Ep20] Loss 0.311\n",
            "[Ep30] Loss 0.309\n",
            "[Ep40] Loss 0.307\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29474   653]\n",
            " [ 3945   928]]\n",
            "Accuracy: 0.8686285714285714\n",
            "Error: 0.13137142857142858\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 100\n",
            "DIMENSIONS = 5\n",
            "[Ep0] Loss 0.477\n",
            "[Ep10] Loss 0.321\n",
            "[Ep20] Loss 0.317\n",
            "[Ep30] Loss 0.316\n",
            "[Ep40] Loss 0.315\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29742   353]\n",
            " [ 4344   561]]\n",
            "Accuracy: 0.8658\n",
            "Error: 0.13419999999999999\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 100\n",
            "DIMENSIONS = 10\n",
            "[Ep0] Loss 0.891\n",
            "[Ep10] Loss 0.321\n",
            "[Ep20] Loss 0.317\n",
            "[Ep30] Loss 0.315\n",
            "[Ep40] Loss 0.314\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29390   729]\n",
            " [ 3956   925]]\n",
            "Accuracy: 0.8661428571428571\n",
            "Error: 0.1338571428571429\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 100\n",
            "DIMENSIONS = 21\n",
            "[Ep0] Loss 0.922\n",
            "[Ep10] Loss 0.317\n",
            "[Ep20] Loss 0.314\n",
            "[Ep30] Loss 0.312\n",
            "[Ep40] Loss 0.311\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29409   757]\n",
            " [ 3843   991]]\n",
            "Accuracy: 0.8685714285714285\n",
            "Error: 0.13142857142857145\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 100\n",
            "DIMENSIONS = 50\n",
            "[Ep0] Loss 6.227\n",
            "[Ep10] Loss 0.320\n",
            "[Ep20] Loss 0.314\n",
            "[Ep30] Loss 0.310\n",
            "[Ep40] Loss 0.308\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29491   651]\n",
            " [ 4004   854]]\n",
            "Accuracy: 0.867\n",
            "Error: 0.133\n",
            "\n",
            "\n",
            "HIDDEN LAYERS = 100\n",
            "DIMENSIONS = 100\n",
            "[Ep0] Loss 11.463\n",
            "[Ep10] Loss 0.322\n",
            "[Ep20] Loss 0.312\n",
            "[Ep30] Loss 0.310\n",
            "[Ep40] Loss 0.310\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29248   886]\n",
            " [ 3833  1033]]\n",
            "Accuracy: 0.8651714285714286\n",
            "Error: 0.1348285714285714\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "dataloader = DataLoader(train_dataset, batch_size=10000, shuffle=True, drop_last=True)\n",
        "accuracies = {}\n",
        "for i in [10, 30, 50, 100]:\n",
        "    for j in [5, 10, 21, 50, 100]:\n",
        "        print(f\"HIDDEN LAYERS = {i}\")\n",
        "        print(f\"DIMENSIONS = {j}\")\n",
        "        experiment_model = BatchNormDNN(num_layers=i, hidden_dim=j)\n",
        "        experiment_model = experiment_model.to(device)\n",
        "        fit(experiment_model, loss, dataloader, epochs=50)\n",
        "        print()\n",
        "        accuracy = evaluate_model(experiment_model, valloader)\n",
        "        print()\n",
        "        print()\n",
        "        accuracies[f\"{i},{j}\"] = accuracy"
      ],
      "id": "Nz5beQ-C6kfn"
    },
    {
      "cell_type": "code",
      "source": [
        "best_key = max(accuracies, key=accuracies.get)\n",
        "best_i, best_j = map(int, best_key.split(','))\n",
        "\n",
        "print(f\"Best hidden layers: {best_i}\")\n",
        "print(f\"Best dimensions: {best_j}\")\n",
        "\n",
        "# Fit the final Model\n",
        "test_model_bn = BatchNormDNN(num_layers=best_i, hidden_dim=best_j).to(device)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=5000, shuffle=True, drop_last=True)\n",
        "fit(test_model_bn, loss, test_dataloader, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fzAjs5eIHcb",
        "outputId": "b61a0ca5-aa9e-4e6a-a656-6026a2257557"
      },
      "id": "7fzAjs5eIHcb",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hidden layers: 10\n",
            "Best dimensions: 10\n",
            "[Ep0] Loss 0.417\n",
            "[Ep10] Loss 0.314\n",
            "[Ep20] Loss 0.310\n",
            "[Ep30] Loss 0.311\n",
            "[Ep40] Loss 0.310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dd301c39",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:20:13.393628700Z",
          "start_time": "2023-12-02T18:20:12.046231400Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd301c39",
        "outputId": "383c553d-4dc2-4b3c-a377-aeecec800275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{BatchNormDNN(\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=21, out_features=10, bias=True)\n",
            "    (1-8): 8 x Linear(in_features=10, out_features=10, bias=True)\n",
            "    (9): Linear(in_features=10, out_features=2, bias=True)\n",
            "  )\n",
            "  (relu): LeakyReLU(negative_slope=0.01)\n",
            "  (batch_norm): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")}\n",
            "Average Test Accuracy: 0.867485693522862\n",
            "Average Test Loss: 0.3056148375783648\n"
          ]
        }
      ],
      "source": [
        "for model in {test_model_bn}:\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=5000, shuffle=True, drop_last=True)\n",
        "    test_error = 0\n",
        "    test_acc= 0\n",
        "    print({model})\n",
        "    for it, batch in enumerate(test_dataloader):\n",
        "        test_inputs = batch[:, :-2]\n",
        "        test_inputs = test_inputs.to(device)\n",
        "        test_labels = batch[:, -2:]\n",
        "        test_labels = test_labels.to(device)\n",
        "        # Compute model predictions\n",
        "        test_pred = model(test_inputs)\n",
        "        l = loss(test_pred, test_labels)\n",
        "        test_acc += (sum(torch.argmax(test_pred, dim=1) == torch.argmax(test_labels, dim=1)) / len(batch)).item()\n",
        "        test_error += l.item()\n",
        "    avg_test_acc = test_acc / len(test_dataloader)\n",
        "    avg_error = test_error / len(test_dataloader)\n",
        "    print(\"Average Test Accuracy:\", avg_test_acc)\n",
        "    print(\"Average Test Loss:\", avg_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c2a9ac895d68833a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-02T18:20:13.716764Z",
          "start_time": "2023-12-02T18:20:13.416569Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2a9ac895d68833a",
        "outputId": "147a628d-37dc-4502-900f-ef632c3452b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[29566   547]\n",
            " [ 4106   781]]\n",
            "Accuracy: 0.8670571428571429\n",
            "Error: 0.13294285714285714\n"
          ]
        }
      ],
      "source": [
        "test_acc = evaluate_model(test_model_bn, test_dataloader)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}